
---
title: "COMPSCIX 415.2 Homework 9/Final"
author: "Phillip Manning"
date: "4/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(rpart)
library(partykit)
library(randomForest)
library(dplyr)
library(ROCR)

```
## Random forests


```{r}
file_path <-'/Users/phillipmanning/Downloads/finalset.csv'
train <- read_csv(file_path)
dim(train)
train <- train %>% mutate(Survived = as.factor(Survived))
train <- train %>% mutate_if(is.character,as.factor)
glimpse(train)
```

### Exercise 1
```{r}
set.seed(987)

model_data <- resample_partition(train, c(test = 0.3, train = 0.7))
train_set <- as.tibble(model_data$train)
test_set <- as.tibble(model_data$test)

```

### Exercise 2
```{r}
Train_mod <- rpart(Survived ~ Pclass + Sex + Fare + Age + Parch + Embarked, data = train_set)
plot(as.party(Train_mod))
```

### Exercise 3 
```{r}
rf_mod <- randomForest(Survived ~ Pclass,
                       data = train_set,
                       ntrees = 500,
                       mtry=4,
                       na.action = na.roughfix)
```
### Exercise 4 
```{r}
rf_preds <- predict(rf_mod, newdata = test_set, type = 'prob')[,2]
tree_preds <- predict(Train_mod, newdata = test_set)[,2]

pred_rf <- prediction (predictions = rf_preds, labels = test_set$Survived)
pred_tree <- prediction (predictions = tree_preds, labels = test_set$Survived)
```
### Exercise 5 
```{r}
# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_rf_tbl) <- c('fpr', 'tpr')

# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')
bind_rows(train_set,test_set)
# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
  geom_line(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') +
  theme_bw()
  return(p)
}
```
### Exercise 6
### The random forest model should perform better than the decision tree because the random forest is taking the average of more features. Although I don't have the specific false positive rate, I can say intuitively that the randomforest model would have a smaller false positive rate than the decision tree. 